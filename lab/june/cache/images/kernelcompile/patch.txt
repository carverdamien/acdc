diff --git a/Makefile b/Makefile
index 0f9cb36..dcd49ae 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 VERSION = 4
 PATCHLEVEL = 6
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = .june
 NAME = Charred Weasel
 
 # *DOCUMENTATION*
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 1191d79..dd46daf 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -70,6 +70,9 @@ enum mem_cgroup_events_index {
 	MEM_CGROUP_EVENTS_PGPGOUT,	/* # of pages paged out */
 	MEM_CGROUP_EVENTS_PGFAULT,	/* # of page-faults */
 	MEM_CGROUP_EVENTS_PGMAJFAULT,	/* # of major page-faults */
+	MEM_CGROUP_EVENTS_PGLOST,	/* # of pages others reclaimed to self */
+	MEM_CGROUP_EVENTS_PGSTOLEN,	/* # of pages reclaimed to others */
+	MEM_CGROUP_EVENTS_PGSELF,       /* # of pages reclaimed to self to protect others */
 	MEM_CGROUP_EVENTS_NSTATS,
 	/* default hierarchy events */
 	MEMCG_LOW = MEM_CGROUP_EVENTS_NSTATS,
@@ -292,6 +295,26 @@ static inline void mem_cgroup_events(struct mem_cgroup *memcg,
 	cgroup_file_notify(&memcg->events_file);
 }
 
+static inline void mem_cgroup_sibling_pressure(struct mem_cgroup *f,
+					       struct mem_cgroup *t,
+					       struct mem_cgroup *r,
+					       unsigned int nr)
+{
+	if (f && t) {
+		if (f != t) {
+			mem_cgroup_events(f,MEM_CGROUP_EVENTS_PGLOST,nr);
+			mem_cgroup_events(t,MEM_CGROUP_EVENTS_PGSTOLEN,nr);
+		} else {
+			if (r!=f) {
+				mem_cgroup_events(f,MEM_CGROUP_EVENTS_PGSELF,nr);
+			}
+		}
+	}
+}
+
+unsigned long mem_cgroup_weight(struct mem_cgroup*, struct zone*);
+unsigned long mem_cgroup_total_weight(struct mem_cgroup*, struct zone*);
+
 bool mem_cgroup_low(struct mem_cgroup *root, struct mem_cgroup *memcg);
 
 int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,
@@ -553,6 +576,23 @@ static inline void mem_cgroup_events(struct mem_cgroup *memcg,
 {
 }
 
+static inline void mem_cgroup_sibling_pressure(struct mem_cgroup *f,
+					       struct mem_cgroup *t,
+					       struct mem_cgroup *r,
+					       unsigned int nr)
+{
+}
+
+static inline unsigned long mem_cgroup_weight(struct mem_cgroup*, struct zone*)
+{
+	return 1;
+}
+
+static inline unsigned long mem_cgroup_total_weight(struct mem_cgroup*, struct zone*)
+{
+	return 1;
+}
+
 static inline bool mem_cgroup_low(struct mem_cgroup *root,
 				  struct mem_cgroup *memcg)
 {
diff --git a/include/linux/swap.h b/include/linux/swap.h
index ad22035..8341bf5 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -320,6 +320,7 @@ extern unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 					gfp_t gfp_mask, nodemask_t *mask);
 extern int __isolate_lru_page(struct page *page, isolate_mode_t mode);
 extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,
+						  struct mem_cgroup *mem_charging,
 						  unsigned long nr_pages,
 						  gfp_t gfp_mask,
 						  bool may_swap);
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index fe787f5..f9758e4 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -113,6 +113,9 @@ static const char * const mem_cgroup_events_names[] = {
 	"pgpgout",
 	"pgfault",
 	"pgmajfault",
+	"pglost",
+	"pgstolen",
+	"pgself",
 };
 
 static const char * const mem_cgroup_lru_names[] = {
@@ -1896,7 +1899,7 @@ static void reclaim_high(struct mem_cgroup *memcg,
 		if (page_counter_read(&memcg->memory) <= memcg->high)
 			continue;
 		mem_cgroup_events(memcg, MEMCG_HIGH, 1);
-		try_to_free_mem_cgroup_pages(memcg, nr_pages, gfp_mask, true);
+		try_to_free_mem_cgroup_pages(memcg, memcg, nr_pages, gfp_mask, true);
 	} while ((memcg = parent_mem_cgroup(memcg)));
 }
 
@@ -1979,7 +1982,7 @@ retry:
 
 	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);
 
-	nr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, nr_pages,
+	nr_reclaimed = try_to_free_mem_cgroup_pages(mem_over_limit, memcg, nr_pages,
 						    gfp_mask, may_swap);
 
 	if (mem_cgroup_margin(mem_over_limit) >= nr_pages)
@@ -2478,7 +2481,7 @@ static int mem_cgroup_resize_limit(struct mem_cgroup *memcg,
 		if (!ret)
 			break;
 
-		try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL, true);
+		try_to_free_mem_cgroup_pages(memcg, memcg, 1, GFP_KERNEL, true);
 
 		curusage = page_counter_read(&memcg->memory);
 		/* Usage is reduced ? */
@@ -2529,7 +2532,7 @@ static int mem_cgroup_resize_memsw_limit(struct mem_cgroup *memcg,
 		if (!ret)
 			break;
 
-		try_to_free_mem_cgroup_pages(memcg, 1, GFP_KERNEL, false);
+		try_to_free_mem_cgroup_pages(memcg, memcg, 1, GFP_KERNEL, false);
 
 		curusage = page_counter_read(&memcg->memsw);
 		/* Usage is reduced ? */
@@ -2654,7 +2657,7 @@ static int mem_cgroup_force_empty(struct mem_cgroup *memcg)
 		if (signal_pending(current))
 			return -EINTR;
 
-		progress = try_to_free_mem_cgroup_pages(memcg, 1,
+		progress = try_to_free_mem_cgroup_pages(memcg, memcg, 1,
 							GFP_KERNEL, true);
 		if (!progress) {
 			nr_retries--;
@@ -3216,10 +3219,37 @@ static int memcg_stat_show(struct seq_file *m, void *v)
 		seq_printf(m, "recent_scanned_file %lu\n", recent_scanned[1]);
 	}
 #endif
-
 	return 0;
 }
 
+unsigned long mem_cgroup_weight(struct mem_cgroup* memcg, struct zone* zone)
+{
+	unsigned long recent_rotated, recent_scanned;
+	struct lruvec *lruvec;
+	struct zone_reclaim_stat *rstat;
+
+	if (!memcg)
+		memcg = root_mem_cgroup;
+	
+	lruvec = mem_cgroup_zone_lruvec(zone, memcg);
+	rstat  = &lruvec->reclaim_stat;
+
+	recent_scanned = 1 + rstat->recent_scanned[0] + rstat->recent_scanned[1];
+	recent_rotated = 1 + rstat->recent_rotated[0] + rstat->recent_rotated[1];
+
+	return recent_scanned / recent_rotated;
+}
+
+unsigned long mem_cgroup_total_weight(struct mem_cgroup* root, struct zone* zone)
+{
+	struct mem_cgroup* memcg;
+	unsigned long total = 0;
+	for_each_mem_cgroup_tree(memcg, root) {
+		total += mem_cgroup_weight(memcg, zone);
+	}
+	return total;
+}
+
 static u64 mem_cgroup_swappiness_read(struct cgroup_subsys_state *css,
 				      struct cftype *cft)
 {
@@ -5003,7 +5033,7 @@ static ssize_t memory_high_write(struct kernfs_open_file *of,
 
 	nr_pages = page_counter_read(&memcg->memory);
 	if (nr_pages > high)
-		try_to_free_mem_cgroup_pages(memcg, nr_pages - high,
+		try_to_free_mem_cgroup_pages(memcg, memcg, nr_pages - high,
 					     GFP_KERNEL, true);
 
 	memcg_wb_domain_size_changed(memcg);
@@ -5057,7 +5087,7 @@ static ssize_t memory_max_write(struct kernfs_open_file *of,
 		}
 
 		if (nr_reclaims) {
-			if (!try_to_free_mem_cgroup_pages(memcg, nr_pages - max,
+			if (!try_to_free_mem_cgroup_pages(memcg, memcg, nr_pages - max,
 							  GFP_KERNEL, true))
 				nr_reclaims--;
 			continue;
diff --git a/mm/rmap.c b/mm/rmap.c
index 307b555..3ebf9c4 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1098,6 +1098,8 @@ void page_move_anon_rmap(struct page *page,
 
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_VMA(!anon_vma, vma);
+	if (IS_ENABLED(CONFIG_DEBUG_VM) && PageTransHuge(page))
+		address &= HPAGE_PMD_MASK;
 	VM_BUG_ON_PAGE(page->index != linear_page_index(vma, address), page);
 
 	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 142cb61..979da5f 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -80,6 +80,11 @@ struct scan_control {
 	 * primary target of this reclaim invocation.
 	 */
 	struct mem_cgroup *target_mem_cgroup;
+	/*
+	 * The memory cgroup that tried to charge memory and as a result
+	 * caused target_mem_cgroup to reach its limit.
+	 */
+	struct mem_cgroup *mem_charging;
 
 	/* Scan (total_size >> priority) pages at once */
 	int priority;
@@ -1967,7 +1972,9 @@ enum scan_balance {
  */
 static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 			   struct scan_control *sc, unsigned long *nr,
-			   unsigned long *lru_pages)
+			   unsigned long *lru_pages,
+			   unsigned long extra_numerator,
+			   unsigned long extra_denominator)
 {
 	int swappiness = mem_cgroup_swappiness(memcg);
 	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
@@ -2161,6 +2168,11 @@ out:
 				BUG();
 			}
 
+			if (scan_balance != SCAN_EQUAL) {
+				scan = div64_u64(scan * extra_numerator, extra_denominator);
+				scan = max(scan, SWAP_CLUSTER_MAX);
+			}
+
 			*lru_pages += size;
 			nr[lru] = scan;
 
@@ -2194,7 +2206,8 @@ static inline void init_tlb_ubc(void)
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
 static void shrink_zone_memcg(struct zone *zone, struct mem_cgroup *memcg,
-			      struct scan_control *sc, unsigned long *lru_pages)
+			      struct scan_control *sc, unsigned long *lru_pages,
+			      unsigned long numerator, unsigned long denominator)
 {
 	struct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);
 	unsigned long nr[NR_LRU_LISTS];
@@ -2206,7 +2219,7 @@ static void shrink_zone_memcg(struct zone *zone, struct mem_cgroup *memcg,
 	struct blk_plug plug;
 	bool scan_adjusted;
 
-	get_scan_count(lruvec, memcg, sc, nr, lru_pages);
+	get_scan_count(lruvec, memcg, sc, nr, lru_pages, numerator, denominator);
 
 	/* Record the original scan target for proportional adjustments later */
 	memcpy(targets, nr, sizeof(nr));
@@ -2391,6 +2404,7 @@ static bool shrink_zone(struct zone *zone, struct scan_control *sc,
 {
 	struct reclaim_state *reclaim_state = current->reclaim_state;
 	unsigned long nr_reclaimed, nr_scanned;
+	unsigned long numerator, denominator;
 	bool reclaimable = false;
 
 	do {
@@ -2405,6 +2419,8 @@ static bool shrink_zone(struct zone *zone, struct scan_control *sc,
 		nr_reclaimed = sc->nr_reclaimed;
 		nr_scanned = sc->nr_scanned;
 
+		denominator = mem_cgroup_total_weight(root,zone);
+
 		memcg = mem_cgroup_iter(root, NULL, &reclaim);
 		do {
 			unsigned long lru_pages;
@@ -2420,9 +2436,16 @@ static bool shrink_zone(struct zone *zone, struct scan_control *sc,
 			reclaimed = sc->nr_reclaimed;
 			scanned = sc->nr_scanned;
 
-			shrink_zone_memcg(zone, memcg, sc, &lru_pages);
+			numerator = mem_cgroup_weight(memcg,zone);
+
+			shrink_zone_memcg(zone, memcg, sc, &lru_pages, numerator, denominator);
 			zone_lru_pages += lru_pages;
 
+			mem_cgroup_sibling_pressure(memcg,
+						    sc->mem_charging,
+						    sc->target_mem_cgroup,
+						    sc->nr_reclaimed - reclaimed);
+
 			if (memcg && is_classzone)
 				shrink_slab(sc->gfp_mask, zone_to_nid(zone),
 					    memcg, sc->nr_scanned - scanned,
@@ -2433,21 +2456,6 @@ static bool shrink_zone(struct zone *zone, struct scan_control *sc,
 				   sc->nr_scanned - scanned,
 				   sc->nr_reclaimed - reclaimed);
 
-			/*
-			 * Direct reclaim and kswapd have to scan all memory
-			 * cgroups to fulfill the overall scan target for the
-			 * zone.
-			 *
-			 * Limit reclaim, on the other hand, only cares about
-			 * nr_to_reclaim pages to be reclaimed and it will
-			 * retry with decreasing priority if one round over the
-			 * whole hierarchy is not sufficient.
-			 */
-			if (!global_reclaim(sc) &&
-					sc->nr_reclaimed >= sc->nr_to_reclaim) {
-				mem_cgroup_iter_break(root, memcg);
-				break;
-			}
 		} while ((memcg = mem_cgroup_iter(root, memcg, &reclaim)));
 
 		/*
@@ -2906,7 +2914,7 @@ unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,
 	 * will pick up pages from other mem cgroup's as well. We hack
 	 * the priority and make it zero.
 	 */
-	shrink_zone_memcg(zone, memcg, &sc, &lru_pages);
+	shrink_zone_memcg(zone, memcg, &sc, &lru_pages, 1, 1);
 
 	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);
 
@@ -2915,6 +2923,7 @@ unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,
 }
 
 unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,
+					   struct mem_cgroup *mem_charging,
 					   unsigned long nr_pages,
 					   gfp_t gfp_mask,
 					   bool may_swap)
@@ -2927,6 +2936,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,
 		.gfp_mask = (gfp_mask & GFP_RECLAIM_MASK) |
 				(GFP_HIGHUSER_MOVABLE & ~GFP_RECLAIM_MASK),
 		.target_mem_cgroup = memcg,
+		.mem_charging = mem_charging,
 		.priority = DEF_PRIORITY,
 		.may_writepage = !laptop_mode,
 		.may_unmap = 1,
